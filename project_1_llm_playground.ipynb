{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feamcor/bytebyteai-cohort-4/blob/main/project_1_llm_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe523821",
      "metadata": {
        "id": "fe523821"
      },
      "source": [
        "# Project 1: Build an LLM Playground\n",
        "\n",
        "Welcome to your first project! In this project, you'll build a simple large language model (LLM) playground, an interactive environment where you can experiment with LLMs and understand how they work under the hood.\n",
        "\n",
        "The goal here is to understand the foundations and mechanics behind LLMs rather than relying on higher-level abstractions or frameworks. You'll see what happens under the hood, how an LLM receives text, processes it, and generates a response. In later projects, you'll use frameworks like `Ollama` and `LangChain` that simplify many of these steps. But before that, this project will help you build a solid mental model of how LLMs actually work.\n",
        "\n",
        "---\n",
        "## Environment Setup\n",
        "We'll use Google Colab, a free browser-based platform that lets you run Python code and machine learning models without installing anything locally. Go to [Google Colab](https://colab.research.google.com/) and upload this notebook to get started."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c145c49a",
      "metadata": {
        "id": "c145c49a"
      },
      "source": [
        "If you prefer to run the project locally, you need a reproducible setup. Open a terminal in the same directory as this notebook and run the environment setup commands below to install dependencies and create an isolated environment.\n",
        "\n",
        "### Step 1: Use Conda or uv to install the project dependencies.\n",
        "\n",
        "#### Option 1: Conda\n",
        "\n",
        "[Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/index.html) is an open-source package and environment manager that lets you create isolated environments and install Python and non-Python dependencies together.\n",
        "\n",
        "```bash\n",
        "# Create and activate the conda environment\n",
        "conda env create -f environment.yaml && conda activate llm_playground\n",
        "```\n",
        "\n",
        "#### Option 2: uv (faster)\n",
        "\n",
        "[uv](https://docs.astral.sh/uv/) (faster) is a fast Python package installer and virtual environment tool written in Rust that aims to replace pip, pip-tools, and virtualenv with a single, high-performance workflow.\n",
        "\n",
        "```bash\n",
        "# Install uv (skip if already installed)\n",
        "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "\n",
        "# Create venv and install dependencies\n",
        "uv venv .venv && source .venv/bin/activate\n",
        "uv pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "### Step 2: Register this environment as a Jupyter kernel\n",
        "This step is optional. Do it only if your environment doesn‚Äôt appear in Jupyter‚Äôs kernel list.\n",
        "```bash\n",
        "python -m ipykernel install --user --name=llm_playground --display-name \"llm_playground\"\n",
        "```\n",
        "\n",
        "Now switch the kernel to `llm_playground` (Kernel ‚Üí Change Kernel)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e82492",
      "metadata": {
        "id": "08e82492"
      },
      "source": [
        "---\n",
        "## Learning Objectives  \n",
        "- Understand tokenization and how raw text is converted into a sequence of discrete tokens\n",
        "- Load and inspect a pretrained LLM (GPT-2) using Hugging Face\n",
        "- Understand logits, probabilities, and how models predict the next token\n",
        "- Count and explore model parameters to understand model scale\n",
        "- Explore decoding strategies: greedy decoding and top-p (nucleus) sampling\n",
        "- See the leap from GPT-2 (simple text completion) to a modern model that understands questions and thinks before answering\n",
        "- Look ahead: inference engines for serving models in practice\n",
        "\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1235110e",
      "metadata": {
        "id": "1235110e"
      },
      "outputs": [],
      "source": [
        "import string, torch, transformers, tiktoken\n",
        "print(\"torch:\\t\\t\", torch.__version__)\n",
        "print(\"transformers:\\t\", transformers.__version__)\n",
        "print(\"tiktoken:\\t\", tiktoken.__version__)\n",
        "print(\"Environment check complete ‚úÖ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c1eb0b",
      "metadata": {
        "id": "d4c1eb0b"
      },
      "source": [
        "# 1: Tokenization\n",
        "\n",
        "A neural network cannot process raw text directly. It needs numbers.\n",
        "Tokenization is the process of converting text into numerical IDs that models can understand. In this section, you will learn how tokenization works in practice and why it is an essential step in every language model pipeline.\n",
        "\n",
        "Tokenization methods generally fall into three main categories:\n",
        "1. Word-level\n",
        "2. Character-level\n",
        "3. Subword-level\n",
        "\n",
        "### 1.1 - Word-level tokenization\n",
        "This method splits text by whitespace and treats each word as a single token. In the next cell, you will implement a basic word-level tokenizer by building a vocabulary that maps words to IDs and writing `encode` and `decode` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d784a288",
      "metadata": {
        "id": "d784a288"
      },
      "outputs": [],
      "source": [
        "CORPUS = [\n",
        "    \"Please do not deploy on friday\",\n",
        "    \"It works on my machine\",\n",
        "    \"Extra spicy noodles are a bad idea!\",\n",
        "    \"Tokens are tiny pieces of text\",\n",
        "]\n",
        "MODEL_GPT2 = \"gpt2\"\n",
        "MODEL_QWEN = \"Qwen/Qwen3-0.6B\"\n",
        "UNKNOWN = \"[UNK]\"\n",
        "SAMPLE_SENTENCE = \"Friday I deploy spicy noodles on tiny pieces of my bad machine ‚úÖ\"\n",
        "SAMPLE_PROMPT = \"I like spicy\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = []\n",
        "word_to_id = {}\n",
        "id_to_word = {}"
      ],
      "metadata": {
        "id": "B5tE80BdvFjH"
      },
      "id": "B5tE80BdvFjH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca1badb9",
      "metadata": {
        "id": "ca1badb9"
      },
      "outputs": [],
      "source": [
        "def tokenize_words():\n",
        "    for sentence in CORPUS:\n",
        "        for word in sentence.lower().split():\n",
        "            if word not in word_to_id:\n",
        "                _id = len(word_to_id)\n",
        "                word_to_id[word] = _id\n",
        "                id_to_word[_id] = word\n",
        "                vocabulary.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b735a7b5",
      "metadata": {
        "id": "b735a7b5"
      },
      "outputs": [],
      "source": [
        "def encode_words(text):\n",
        "    ids = []\n",
        "    for word in text.lower().split():\n",
        "        if word in word_to_id:\n",
        "            ids.append(word_to_id[word])\n",
        "        else:\n",
        "            ids.append(-1)\n",
        "    return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a0167d",
      "metadata": {
        "id": "72a0167d"
      },
      "outputs": [],
      "source": [
        "def decode_words(ids):\n",
        "    words = []\n",
        "    for _id in ids:\n",
        "        if _id in id_to_word:\n",
        "            words.append(id_to_word[_id])\n",
        "        else:\n",
        "            words.append(UNKNOWN)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a4c75e",
      "metadata": {
        "id": "29a4c75e"
      },
      "outputs": [],
      "source": [
        "tokenize_words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6efce78",
      "metadata": {
        "id": "e6efce78"
      },
      "outputs": [],
      "source": [
        "print(vocabulary)\n",
        "print(word_to_id)\n",
        "print(id_to_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb614b9e",
      "metadata": {
        "id": "cb614b9e"
      },
      "outputs": [],
      "source": [
        "test_words = encode_words(SAMPLE_SENTENCE)\n",
        "print(test_words)\n",
        "print(decode_words(test_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edab2c2",
      "metadata": {
        "id": "0edab2c2"
      },
      "source": [
        "While word-level tokenization is simple and easy to understand, it has two key limitations that make it impractical for large-scale models:\n",
        "1. Large vocabulary size: every new word or variation (for example, run, runs, running) increases the total vocabulary, leading to higher memory and training costs.\n",
        "2. Out-of-vocabulary (OOV) problem: the model cannot handle unseen or rare words that were not part of the training vocabulary, so they must be replaced with a generic [UNK] token.\n",
        "\n",
        "The next section introduces character-level tokenization, where text is represented as individual characters instead of words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a379bac7",
      "metadata": {
        "id": "a379bac7"
      },
      "source": [
        "### 1.2 - Character-level tokenization\n",
        "\n",
        "In this approach, every single character (including spaces, punctuation, and even emojis) is assigned its own ID.\n",
        "\n",
        "In the next cell, we will rebuild a tokenizer using the same corpus as before, but this time with a character-level approach.\n",
        "For simplicity, we will only use English letters (a-z, A-Z) and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac29144",
      "metadata": {
        "id": "4ac29144"
      },
      "outputs": [],
      "source": [
        "vocabulary = []\n",
        "char_to_id = {}\n",
        "id_to_char = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9456ffff",
      "metadata": {
        "id": "9456ffff"
      },
      "outputs": [],
      "source": [
        "def tokenize_chars():\n",
        "    for sentence in CORPUS:\n",
        "        for char in sentence.lower():\n",
        "            if char not in char_to_id:\n",
        "                _id = len(char_to_id)\n",
        "                char_to_id[char] = _id\n",
        "                id_to_char[_id] = char\n",
        "                vocabulary.append(char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4baab303",
      "metadata": {
        "id": "4baab303"
      },
      "outputs": [],
      "source": [
        "def encode_chars(text):\n",
        "    ids = []\n",
        "    for char in text.lower():\n",
        "        if char in char_to_id:\n",
        "            ids.append(char_to_id[char])\n",
        "        else:\n",
        "            ids.append(-1)\n",
        "    return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66aca74f",
      "metadata": {
        "id": "66aca74f"
      },
      "outputs": [],
      "source": [
        "def decode_chars(ids):\n",
        "    chars = []\n",
        "    for _id in ids:\n",
        "        if _id in id_to_char:\n",
        "            chars.append(id_to_char[_id])\n",
        "        else:\n",
        "            chars.append(UNKNOWN)\n",
        "    return chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e694342b",
      "metadata": {
        "id": "e694342b"
      },
      "outputs": [],
      "source": [
        "tokenize_chars()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f66ac5b",
      "metadata": {
        "id": "6f66ac5b"
      },
      "outputs": [],
      "source": [
        "print(vocabulary)\n",
        "print(char_to_id)\n",
        "print(id_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ede2771",
      "metadata": {
        "id": "6ede2771"
      },
      "outputs": [],
      "source": [
        "test_chars = encode_chars(SAMPLE_SENTENCE)\n",
        "print(test_chars)\n",
        "print(decode_chars(test_chars))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f7e393",
      "metadata": {
        "id": "25f7e393"
      },
      "source": [
        "Character-level tokenization solves the out-of-vocabulary problem but introduces new challenges:\n",
        "\n",
        "1. Longer sequences: because each word becomes many tokens, models need to process much longer inputs.\n",
        "2. Weaker semantic representation: individual characters carry very little meaning, so models must learn relationships across many steps.\n",
        "3. Higher computational cost: longer sequences lead to more tokens per input, which increases training and inference time.\n",
        "\n",
        "To find a better balance between vocabulary size and sequence length, we move to subword-level tokenization next."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391275bd",
      "metadata": {
        "id": "391275bd"
      },
      "source": [
        "### 1.3 - Subword-level tokenization\n",
        "\n",
        "Subword methods such as `Byte-Pair Encoding (BPE)`, `WordPiece`, and `SentencePiece` **learn** common groups of characters and merge them into tokens. For example, the word **unbelievable** might turn into three tokens: **[\"un\", \"believ\", \"able\"]**. This approach strikes a balance between word-level and character-level methods and fixes their limitations.\n",
        "\n",
        "The BPE algorithm builds a vocabulary iteratively using the following process:\n",
        "1. Start with individual characters or bytes (each character is a token).\n",
        "2. Count all adjacent pairs of tokens in a large text CORPUS.\n",
        "3. Merge the most frequent pair into new tokens.\n",
        "\n",
        "Repeat steps 2 and 3 until you reach the desired vocabulary size (for example, 50,000 tokens).\n",
        "\n",
        "In the next cell, you will experiment with BPE in practice to see how it compresses text into meaningful subword units. Instead of implementing the algorithm from scratch, you will use a pretrained tokenizer, which was already trained on a large text corpus to build its vocabulary, such as the data used to train `GPT-2`. This allows you to see how BPE works in practice with a real, learned vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4675e67a",
      "metadata": {
        "id": "4675e67a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_GPT2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55325562",
      "metadata": {
        "id": "55325562"
      },
      "outputs": [],
      "source": [
        "input_ids = tokenizer.encode(SAMPLE_SENTENCE)\n",
        "print(\"Input IDs:\\t\", input_ids)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(\"BPE Tokens:\\t\", tokens)\n",
        "\n",
        "decoded_text = tokenizer.decode(input_ids)\n",
        "print(\"Decoded:\\t\", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "badaa5a8",
      "metadata": {
        "id": "badaa5a8"
      },
      "source": [
        "### 1.4 - TikToken\n",
        "\n",
        "`tiktoken` is a fast, production-ready library for tokenization used by OpenAI models.\n",
        "It is designed for efficiency and consistency with how OpenAI counts tokens in GPT models.\n",
        "\n",
        "In this section, you will explore how different model families use different tokenizers. We will compare tokenizers used to train `GPT-2` and more powerful models such as `GPT-4`. By trying both, you will see how tokenization has evolved to handle more diverse text (including emojis and special characters) while remaining efficient.\n",
        "\n",
        "In the next cell, you will use tiktoken to load these encodings and inspect how each one splits the same text. You may find reading this doc helpful: https://github.com/openai/tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7704c470",
      "metadata": {
        "id": "7704c470"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "gpt2_enc = tiktoken.get_encoding(MODEL_GPT2)\n",
        "gpt4_enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "gpt2_tokens = gpt2_enc.encode(SAMPLE_SENTENCE)\n",
        "gpt4_tokens = gpt4_enc.encode(SAMPLE_SENTENCE)\n",
        "\n",
        "print(f\"GPT-2 ({len(gpt2_tokens)} tokens):\\t{gpt2_tokens}\")\n",
        "print(f\"GPT-4 ({len(gpt4_tokens)} tokens):\\t{gpt4_tokens}\")\n",
        "\n",
        "print(f\"GPT-2 Special Tokens:\\t{gpt2_enc.special_tokens_set}\")\n",
        "print(f\"GPT-4 Special Tokens:\\t{gpt4_enc.special_tokens_set}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8c1023",
      "metadata": {
        "id": "5e8c1023"
      },
      "source": [
        "Try changing the input sentence and observe how different tokenizers behave.\n",
        "Experiment with:\n",
        "- Emojis, special characters, or punctuation\n",
        "- Code snippets or structured text\n",
        "- Non-English text (for example, Japanese, French, or Arabic)\n",
        "\n",
        "If you are curious, you can also attempt to implement the BPE algorithm yourself using a small text corpus to see how token merges are learned in practice.\n",
        "\n",
        "### 1.5 - Key Takeaways\n",
        "- **Word-level**: simple and intuitive, but limited by large vocabularies and out-of-vocabulary issues\n",
        "- **Character-level**: flexible and covers all text, but produces long sequences that are harder to model\n",
        "- **Subword / BPE**: balances both worlds and is the default choice for most modern LLMs\n",
        "- **TikToken**: a production-ready tokenizer used in OpenAI models, demonstrating how optimized subword vocabularies are applied in real systems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a758ba",
      "metadata": {
        "id": "c2a758ba"
      },
      "source": [
        "# 2: What is a Language Model?\n",
        "\n",
        "At its core, a **language model** is a function that predicts the next token. Given a sequence of tokens `[t‚ÇÅ, t‚ÇÇ, ‚Ä¶, t‚Çô]`, it outputs probabilities for the next token `t‚Çô‚Çä‚ÇÅ`.\n",
        "\n",
        "Models like GPT-2 use many stacked Transformer layers. Each layer mixes information between tokens (attention) and transforms it (feed-forward). Together, these layers learn patterns in text. At the end, the model produces logits: one score per token in the vocabulary. Higher logits mean the token is more likely to be next.\n",
        "\n",
        "In this section, you‚Äôll load GPT-2, look at its architecture, count its parameters, and see how it predicts the next token."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04f56bf",
      "metadata": {
        "id": "a04f56bf"
      },
      "source": [
        "### 2.1: Loading GPT-2\n",
        "\n",
        "There are different ways to load and run pretrained language models.\n",
        "\n",
        "In this project, we‚Äôll use Hugging Face Transformers, a popular Python library that makes it easy to download models like GPT-2 and run them locally.\n",
        "\n",
        "There are also dedicated inference engines for serving and running modern LLMs more efficiently, such as **Ollama**, **SGLang**, and **vLLM**. We‚Äôll explore those in future projects.\n",
        "\n",
        "In the next cell, you‚Äôll load GPT-2 and inspect its architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47c87f6e",
      "metadata": {
        "id": "47c87f6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_GPT2)\n",
        "\n",
        "print(\"\\n==> FULL MODEL ARCHITECTURE <==\")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n==> FIRST TRANSFORMER BLOCK <==\")\n",
        "print(model.transformer.h[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xirey8d3vbk",
      "metadata": {
        "id": "xirey8d3vbk"
      },
      "source": [
        "### 2.2: Counting Parameters\n",
        "\n",
        "You often hear that an LLM has a few million or billion parameters. But what does that actually mean? Every weight and bias value inside every layer is a parameter. These are the numbers that the model learned during training.\n",
        "\n",
        "Next, you will count the total number of parameters in GPT-2 and break them down by component to see where most of the model's capacity lives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2hoy7kku2y8",
      "metadata": {
        "id": "2hoy7kku2y8"
      },
      "outputs": [],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total Parameters in GPT-2:\\t{total_params:,}\")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable Parameters:\\t\\t{trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89mpckm6d",
      "metadata": {
        "id": "e89mpckm6d"
      },
      "source": [
        "**Think about scale:** GPT-2 Small has 124M parameters. GPT-4 is estimated to have over 1 trillion. If each parameter is a 16-bit floating point number (2 bytes), how much memory would you need just to store GPT-2 in RAM? What about a 70B parameter model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6a7495",
      "metadata": {
        "id": "0a6a7495"
      },
      "source": [
        "### 2.3: From Text to Predictions\n",
        "\n",
        "When you pass a sequence of tokens through a language model, it produces a tensor of logits with shape\n",
        "`(batch_size, seq_len, vocab_size)`.\n",
        "Each position in the sequence receives a vector of scores representing how likely every possible token is to appear next. By applying a softmax function on the last dimension, these logits can be converted into probabilities that sum to 1.\n",
        "\n",
        "In the next cell, you will feed a sentence into GPT-2, print the shape of its logits, and display the five most likely next tokens predicted for the final position in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98b7b34",
      "metadata": {
        "id": "f98b7b34"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_GPT2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37894624",
      "metadata": {
        "id": "37894624"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(SAMPLE_PROMPT, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf40003",
      "metadata": {
        "id": "ddf40003"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "print(f\"Logits shape: \\t{logits.shape}\") # (batch_size, seq_len, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "527da6c6",
      "metadata": {
        "id": "527da6c6"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "next_token_logits = logits[0, -1, :]\n",
        "\n",
        "probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "top_k = 5\n",
        "top_probs, top_indices = torch.topk(probs, top_k)\n",
        "\n",
        "print(f\"Prompt:\\t{SAMPLE_PROMPT}\")\n",
        "print(\"Top 5 predicted next tokens:\")\n",
        "for i in range(top_k):\n",
        "    token = tokenizer.decode([top_indices[i]])\n",
        "    print(f\"{token}\\t{top_probs[i].item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb05c9b",
      "metadata": {
        "id": "0eb05c9b"
      },
      "source": [
        "### 2.4: Key Takeaway\n",
        "\n",
        "A language model is not a black box or something mysterious.\n",
        "It is a large composition of simple, understandable layers such as attention and feed-forward networks, trained together to predict the next token in a sequence.\n",
        "\n",
        "By learning this next-token prediction task at scale, the model gradually develops an internal understanding of language structure, meaning, and context, which allows it to generate coherent and relevant text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ccf391",
      "metadata": {
        "id": "e0ccf391"
      },
      "source": [
        "# 3: Text Generation (Decoding)\n",
        "Once a language model can predict the next-token probabilities, we can use it to generate text. This is called text generation or decoding.\n",
        "\n",
        "Conceptually, generation is a loop:\n",
        "\n",
        "1. Feed the current token sequence into the model.\n",
        "\n",
        "2. The model outputs a probability distribution over the next token.\n",
        "\n",
        "3. A decoding algorithm picks the next token from that distribution.\n",
        "\n",
        "4. Append the chosen token to the sequence and repeat.\n",
        "\n",
        "In practice, libraries like Hugging Face Transformers provide generate() methods that handle this loop for you, including stopping conditions, batching, and efficiency tricks.\n",
        "\n",
        "Different decoding strategies control how the next token is chosen, and how creative or deterministic the output feels:\n",
        "\n",
        "- **Greedy decoding**: always pick the token with the highest probability. Simple and consistent, but often repetitive.\n",
        "\n",
        "- **Top-p (nucleus) sampling**: randomly sample from the smallest set of tokens whose cumulative probability exceeds a threshold p. This adds variety while keeping outputs coherent.\n",
        "\n",
        "In the following sections, you'll use generate() to produce text from GPT-2 using both greedy decoding and top-p sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0c5728",
      "metadata": {
        "id": "ac0c5728"
      },
      "source": [
        "### 3.1: Greedy Decoding\n",
        "In this section, you will use GPT-2 and Hugging Face's built-in generate method to produce text using the greedy decoding strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2cb953",
      "metadata": {
        "id": "2f2cb953"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_GPT2)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_GPT2)\n",
        "\n",
        "inputs = tokenizer(SAMPLE_PROMPT, return_tensors=\"pt\")\n",
        "\n",
        "# Greedy decoding is the default when no sampling is specified\n",
        "output_tokens = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
        "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe777ba",
      "metadata": {
        "id": "dbe777ba"
      },
      "outputs": [],
      "source": [
        "tests = [\"Once upon a time\", \"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "\n",
        "def generate_greedy(model, tokenizer, prompt, max_new_tokens=32):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "for prompt in tests:\n",
        "    print(\"\\n==> GPT-2 | Greedy <==\")\n",
        "    print(generate_greedy(model, tokenizer, prompt, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f51d44b2",
      "metadata": {
        "id": "f51d44b2"
      },
      "source": [
        "Naively selecting the single most probable token at each step (known as greedy decoding) often leads to poor results in practice:\n",
        "- Repetition loops: phrases like \"The cat is is is‚Ä¶\"\n",
        "- Short-sighted choices: the most likely token right now might lead to incoherent text later\n",
        "\n",
        "These issues are why more advanced decoding methods such as top-p (nucleus) sampling are commonly used to make model outputs more diverse and natural."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91607661",
      "metadata": {
        "id": "91607661"
      },
      "source": [
        "### 3.2: Top-p (Nucleus) Sampling\n",
        "Top-p sampling (also called nucleus sampling) introduces controlled randomness into text generation. Instead of always picking the single most likely token, it samples from the smallest set of tokens whose cumulative probability exceeds a threshold `p` (e.g., 0.9).\n",
        "\n",
        "This allows the model to explore multiple plausible continuations, producing more diverse and natural-sounding text while still staying coherent.\n",
        "\n",
        "In this section, you will implement a generate function that supports both greedy and top-p strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0633d4a3",
      "metadata": {
        "id": "0633d4a3"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(SAMPLE_PROMPT, return_tensors=\"pt\")\n",
        "\n",
        "# top_p=0.9 means we sample from the top 90% of the probability mass\n",
        "output_tokens = model.generate(**inputs, max_new_tokens=32, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ca8865",
      "metadata": {
        "id": "24ca8865"
      },
      "outputs": [],
      "source": [
        "def generate_top_p(model, tokenizer, prompt, strategy=\"top_p\", max_new_tokens=128):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    if strategy == \"top_p\":\n",
        "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, top_p=0.9, temperature=0.8)\n",
        "    else:\n",
        "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "tests=[\"Once upon a time\",\"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(\"\\n==> GPT-2 | Top-p <==\")\n",
        "    print(generate_top_p(model, tokenizer, prompt, \"top_p\", 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b775b02",
      "metadata": {
        "id": "6b775b02"
      },
      "source": [
        "# 4: From Simple GPT2 to Modern LLMs\n",
        "\n",
        "So far, we have used **GPT-2**, one of the earliest publicly available language models (released in 2019, 124M parameters). GPT-2 can only do one thing: **complete text**. Given some input, it predicts what words come next. It has no concept of questions, instructions, or conversation. If you type `\"What is 2+2?\"`, GPT-2 will continue the text as if it were part of a web page or article. It does not understand you are asking a question.\n",
        "\n",
        "Modern language models are a different. Models like **Qwen3-0.6B** (2025, 600M parameters) have gone through additional training stages that unlock fundamentally new capabilities:\n",
        "\n",
        "- **Instruction following**: they interpret your input as a request and produce a helpful response\n",
        "- **Conversation**: they maintain context across a multi-turn dialogue\n",
        "- **Reasoning**: they can *think step-by-step* before answering, using internal reasoning tokens (`<think>...</think>`) to work through a problem before giving a final answer\n",
        "\n",
        "In this section, you will see this dramatic contrast firsthand by giving the same prompts to both models. In week 4, we'll learn about thinking and reasoning models in detail.\n",
        "\n",
        "### 4.1: Chat Templates\n",
        "\n",
        "Instruction-tuned models expect input in a structured **chat format**, not raw text. Instead of receiving `\"What is 2+2?\"` as plain text, the model expects a formatted message like:\n",
        "\n",
        "```\n",
        "<|user|>What is 2+2?<|assistant|>\n",
        "```\n",
        "\n",
        "Each model family defines its own format. The Hugging Face `tokenizer.apply_chat_template()` method handles this formatting automatically. Without it, even an instruction-tuned model receives unstructured text and falls back to simple completion behavior.\n",
        "\n",
        "### 4.2: GPT-2 vs. Qwen3-0.6B\n",
        "\n",
        "In the next cells, you will load both models and feed the same prompt to each one:\n",
        "\n",
        "- **GPT-2**: receives the raw prompt and blindly continues the text\n",
        "- **Qwen3-0.6B**: receives a properly formatted chat message, *thinks* through the problem, and produces a direct answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b73e7a",
      "metadata": {
        "id": "57b73e7a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(MODEL_GPT2)\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(MODEL_GPT2)\n",
        "\n",
        "qwen_model = AutoModelForCausalLM.from_pretrained(MODEL_QWEN)\n",
        "qwen_tokenizer = AutoTokenizer.from_pretrained(MODEL_QWEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef49ab1b",
      "metadata": {
        "id": "ef49ab1b"
      },
      "source": [
        "Both models are now loaded. GPT-2 has 124M parameters; Qwen3-0.6B has roughly 600M. If the previous cell took some time, that was mainly due to model download. The models are cached locally, so future runs will be much faster.\n",
        "\n",
        "Next, we will generate text from both models using the same prompt. For GPT-2, we pass the raw text directly. For Qwen, we first format the prompt as a chat message using `apply_chat_template()`, then generate. Both use **top-p sampling** so the outputs are varied and natural."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c78a508",
      "metadata": {
        "id": "0c78a508"
      },
      "outputs": [],
      "source": [
        "prompt = \"What is 2+2?\"\n",
        "\n",
        "print(\"==> GPT-2 Output <==\")\n",
        "inputs_gpt2 = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
        "output_gpt2 = gpt2_model.generate(**inputs_gpt2, max_new_tokens=32, do_sample=True, top_p=0.9)\n",
        "print(gpt2_tokenizer.decode(output_gpt2[0], skip_special_tokens=True))\n",
        "\n",
        "print(\"\\n==> Qwen3 Output <==\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = qwen_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "inputs_qwen = qwen_tokenizer([text], return_tensors=\"pt\")\n",
        "output_qwen = qwen_model.generate(**inputs_qwen, max_new_tokens=64, do_sample=True, top_p=0.9)\n",
        "\n",
        "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs_qwen.input_ids, output_qwen)]\n",
        "print(qwen_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "237b6f72",
      "metadata": {
        "id": "237b6f72"
      },
      "source": [
        "# 5. (Optional) A Small Interactive LLM Playground\n",
        "This section is optional. You do not need to implement it to complete the project. It is meant purely for exploration and will not significantly affect your core AI engineering skills.\n",
        "\n",
        "If you are curious, you can build a simple interactive playground to experiment with text generation. You can:\n",
        "- Create input widgets for the prompt, model selection, decoding strategy, and temperature\n",
        "- Use Hugging Face's generate method to produce text based on the selected settings\n",
        "- Display the model's response directly in the notebook output\n",
        "\n",
        "You may find following links helpful:\n",
        "- https://ipywidgets.readthedocs.io/en/latest/\n",
        "- https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed05ceb",
      "metadata": {
        "id": "fed05ceb"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "prompt_input = widgets.Textarea(value='What is the meaning of life?', placeholder='Type your prompt here...', description='Prompt:', layout={'width': '90%', 'height': '100px'})\n",
        "model_dropdown = widgets.Dropdown(options=[('GPT-2 (Completion)', 'gpt2'), ('Qwen (Instruction)', 'qwen')], value='qwen', description='Model:')\n",
        "strategy_dropdown = widgets.Dropdown(options=[('Top-p Sampling', 'top_p'), ('Greedy', 'greedy')], value='top_p', description='Strategy:')\n",
        "temp_slider = widgets.FloatSlider(value=0.7, min=0.1, max=1.5, step=0.1, description='Temp:')\n",
        "tokens_slider = widgets.IntSlider(value=64, min=16, max=256, step=16, description='Max Tokens:')\n",
        "generate_btn = widgets.Button(description='Generate', button_style='primary')\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def run_generation(b):\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(\"Generating...\")\n",
        "\n",
        "        current_model = qwen_model if model_dropdown.value == 'qwen' else gpt2_model\n",
        "        current_tok = qwen_tokenizer if model_dropdown.value == 'qwen' else gpt2_tokenizer\n",
        "\n",
        "        text = prompt_input.value\n",
        "        if model_dropdown.value == 'qwen':\n",
        "            messages = [{\"role\": \"user\", \"content\": text}]\n",
        "            text = current_tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        inputs = current_tok(text, return_tensors=\"pt\")\n",
        "\n",
        "        do_sample = strategy_dropdown.value == 'top_p'\n",
        "        top_p = 0.9 if do_sample else None\n",
        "\n",
        "        outputs = current_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=tokens_slider.value,\n",
        "            do_sample=do_sample,\n",
        "            top_p=top_p,\n",
        "            temperature=temp_slider.value,\n",
        "            pad_token_id=current_tok.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Remove input tokens from output\n",
        "        gen_ids = outputs[0][len(inputs['input_ids'][0]):]\n",
        "        response = current_tok.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "        clear_output()\n",
        "        display(Markdown(f\"### Response:\\n{response}\"))\n",
        "\n",
        "generate_btn.on_click(run_generation)\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h2>LLM Playground</h2>\"),\n",
        "    prompt_input,\n",
        "    widgets.HBox([model_dropdown, strategy_dropdown]),\n",
        "    widgets.HBox([temp_slider, tokens_slider]),\n",
        "    generate_btn,\n",
        "    output_area\n",
        "])\n",
        "display(ui)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ynqgttx8gzj",
      "metadata": {
        "id": "ynqgttx8gzj"
      },
      "source": [
        "# 6: Inference Engines: Ollama, vLLM, SGLang\n",
        "\n",
        "So far, we loaded models directly in Python using HuggingFace's `transformers` library. This is great for learning, but in practice models run as **servers** that expose an API. Client applications send requests and receive responses over HTTP ‚Äî the model itself stays loaded in memory (and on the GPU) between requests.\n",
        "\n",
        "An **inference engine** handles all the heavy lifting: model loading, GPU memory management, request batching, and serving an HTTP API. Popular inference engines include:\n",
        "\n",
        "| Engine | Best for |\n",
        "|--------|----------|\n",
        "| **Ollama** | Easy local use and experimentation |\n",
        "| **vLLM** | High-throughput production serving |\n",
        "| **SGLang** | Fast serving + structured output |\n",
        "\n",
        "Most inference engines expose an **OpenAI-compatible API**. This means you can learn one client library (the `openai` Python package) and swap backends freely: Ollama for local development, vLLM or SGLang for production.\n",
        "\n",
        "In future weeks, we'll learn about Ollama, set it up, and use it to easily load and build on top of modern powerful LLMs!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbccead",
      "metadata": {
        "id": "cfbccead"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've just explored the internals of a real **LLM**. In this project you:\n",
        "* Learned how **tokenization** works ‚Äî from word-level to BPE ‚Äî and why it matters\n",
        "* Used `tiktoken` to compare tokenizers across different model generations\n",
        "* Loaded GPT-2 and inspected its Transformer blocks and layers\n",
        "* **Counted parameters** and understood where a model's capacity lives\n",
        "* Learned how the model produces **logits and probabilities** to predict the next token\n",
        "* Explored **decoding strategies**: greedy decoding and top-p (nucleus) sampling\n",
        "* Witnessed the leap from GPT-2 (simple text completion) to Qwen3-0.6B ‚Äî a modern model that **understands questions and thinks before answering**\n",
        "* Learned about **inference engines** (Ollama, vLLM, SGLang) and the OpenAI-compatible API pattern\n",
        "\n",
        "üëè **Great job!** Take a moment to celebrate. You now have a working mental model of how LLMs work ‚Äî from raw text input all the way to generated output. The skills and intuitions you built here will serve as the foundation for everything that comes next.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}